import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch_geometric.nn import GATConv

"""DAG æœç´¢ç©ºé—´ï¼ˆå³è¶…ç½‘ï¼‰"""
# conv1 = GATConv(in_channels=64, out_channels=64, heads=1, add_self_loops=False,bias=True,dropout=0.2).cuda()
# class Op(nn.Module):
#     '''ğ‘¯ï¼ˆ3)å’Œğ‘¯(4ï¼‰æœ‰å¤šä¸ªä¼ å…¥é“¾æ¥ï¼Œå®ƒå°†å¤šä¸ªä¼ æ’­è·¯å¾„ç»„åˆåœ¨ä¸€èµ·ã€‚è¿™ä¸èƒ½é€šè¿‡GTN[34]æ¥å®ç°ï¼Œå®ƒåªé€šè¿‡çŸ©é˜µä¹˜æ³•æ¥å­¦ä¹ å…ƒè·¯å¾„ã€‚'''
#     def __init__(self):
#         super(Op, self).__init__()
#     def forward(self, x, adjs, ws, idx):
#         #assert(ws.size(0) == len(adjs))
#         if(adjs[idx].size(0) == 2):
#             return ws[idx] * conv1(x, adjs[idx])
#         elif(adjs[idx].size(0) != 2):
#             return ws[idx] * torch.spmm(adjs[idx], x)

class Op(nn.Module):
    def __init__(self):
        super(Op, self).__init__()

    def forward(self, x, adjs, ws, idx):
        return  ws[idx]*torch.spmm(adjs[idx], x)


class Cell(nn.Module):
    '''
    the DAG search space
    '''
    #use_norm:å¯¹éšè—å±‚å½’ä¸€åŒ–
    def __init__(self, n_step, n_hid_prev, n_hid, cstr, use_norm = True, use_nl = True):
        super(Cell, self).__init__()
        #æ˜ å°„ç‰¹å¾ç»´åº¦
        self.affine = nn.Linear(n_hid_prev, n_hid)
        self.n_step = n_step               #* ä¸­é—´çŠ¶æ€çš„æ•°é‡ï¼ˆå³Kï¼‰ã€‚
        #LayerNormå®é™…å°±æ˜¯å¯¹éšå«å±‚åšå±‚å½’ä¸€åŒ–ï¼Œå³å¯¹æŸä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒçš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ã€‚ï¼ˆæ¯hidden_sizeä¸ªæ•°æ±‚å¹³å‡/æ–¹å·®ï¼‰
        self.norm = nn.LayerNorm(n_hid, elementwise_affine = False) if use_norm is True else lambda x : x
        self.use_nl = use_nl
        #å¦‚æœè¦åˆ¤æ–­ä¸¤ä¸ªç±»å‹æ˜¯å¦ç›¸åŒæ¨èä½¿ç”¨ isinstance()
        assert(isinstance(cstr, list))
        self.cstr = cstr                   #* ç±»å‹çº¦æŸ

        #A âˆª {ğ‘°}
        self.ops_seq = nn.ModuleList()     #* state (i - 1) -> state i, 1 <= i < K
        for i in range(1, self.n_step):
            self.ops_seq.append(Op())

        #A âˆª {ğ‘°} âˆª {ğ‘¶}
        self.ops_res = nn.ModuleList()     #* state j -> state i, 0 <= j < i - 1, 2 <= i < K
        for i in range(2, self.n_step):
            for j in range(i - 1):
                self.ops_res.append(Op())

        #A-
        self.last_seq = Op()               #* state (K - 1) -> state K
        #A- âˆª {ğ‘°} âˆª {ğ‘¶}
        self.last_res = nn.ModuleList()    #* state i -> state K, 0 <= i < K - 1
        for i in range(self.n_step - 1):
            self.last_res.append(Op())
    

    def forward(self, x, adjs, ws_seq, idxes_seq, ws_res, idxes_res):
        #assert(isinstance(ws_seq, list))
        #assert(len(ws_seq) == 2)
        #1.æ˜ å°„æŒ‡å®šç»´åº¦
        x = self.affine(x)
        #2.å›¾çš„çŠ¶æ€
        states = [x]
        # offsetèŠ‚ç‚¹ç±»å‹
        offset = 0
        for i in range(self.n_step - 1):
            # Aå¯¹æŒ‡å®šè¾¹ç¼˜ç±»å‹ï¼ˆindex_seqï¼‰è¿›è¡Œæ¶ˆæ¯ä¼ é€’,offset:èŠ‚ç‚¹ç±»å‹
            # idxes_seq:é€‰å–å“ªä¸€ä¸ªçŸ©é˜µ(A âˆª {ğ‘° })
            seqi = self.ops_seq[i](states[i], adjs[:-1], ws_seq[0][i], idxes_seq[0][i])  # ! æ’é™¤é›¶è¿ç®—
            # A âˆª {ğ‘° } âˆª {ğ‘¶}
            resi = sum(self.ops_res[offset + j](h, adjs, ws_res[0][offset + j], idxes_res[0][offset + j]) for j, h in
                       enumerate(states[:i]))
            offset += i
            states.append(seqi + resi)
        # assert(offset == len(self.ops_res))

        #çº¦æŸçŸ©é˜µ->æ¶ˆæ¯ä¼ æ’­
        adjs_cstr = [adjs[i] for i in self.cstr]#é€‰å–å“ªä¸€ä¸ªé‚»æ¥çŸ©é˜µ
        #å–æœ€åä¸€ä¸ªå›¾çš„çŠ¶æ€æ›´æ–°(A-)
        out_seq = self.last_seq(states[-1], adjs_cstr, ws_seq[1], idxes_seq[1])
        #æ·»åŠ ç©ºçŸ©é˜µ(A- âˆª {ğ‘° } âˆª {ğ‘¶})
        adjs_cstr.append(adjs[-1])
        out_res = sum(self.last_res[i](h, adjs_cstr, ws_res[1][i], idxes_res[1][i]) for i, h in enumerate(states[:-1]))
        # æ±‚èŒƒæ•°ï¼ˆ1æ±‚ç»å¯¹å€¼ï¼Œ2æ±‚å¹³æ–¹å’Œå¼€å¹³æ–¹ï¼Œ3æ±‚å¹³æ–¹å’Œå¼€ç«‹æ–¹æ ¹ï¼‰
        output = self.norm(out_seq + out_res)
        # geluå‡½æ•°æ¿€æ´»
        # if self.use_nl:
        #     output = F.gelu(output)
        return output


class Model(nn.Module):
    #n_stepsï¼šæˆ‘ä»¬è¾“å…¥çš„é•¿åº¦
    #cstrï¼›preprocessä¸­å®šä¹‰å¤§çš„ç±»å‹çº¦æŸ
    def __init__(self, in_dims, n_hid, n_adjs, n_steps, cstr, attn_dim = 64, use_norm = True, out_nl = True):
        super(Model, self).__init__()
        #1.ç¥ç»ç½‘ç»œåˆå§‹åŒ–
        self.cstr = cstr #ç±»å‹çº¦æŸ
        self.n_adjs = n_adjs #é‚»æ¥çŸ©é˜µ6ä¸ª(åˆå§‹åŒ–A)
        self.n_hid = n_hid #éšè—å±‚64ç»´
        self.ws = nn.ModuleList() #æƒé‡çŸ©é˜µ(åˆå§‹åŒ–nw)
        #* èŠ‚ç‚¹ç±»å‹çš„ç‰¹å®šè½¬æ¢
        # in_dims[708,1512]
        assert(isinstance(in_dims, list))
        for i in range(len(in_dims)):
            # 708->64
            # 1512->64
            self.ws.append(nn.Linear(in_dims[i], n_hid))
        assert(isinstance(n_steps, list))

        #2.è¯ç‰©é¶æ ‡DAG
        self.metas = nn.ModuleList()
        #ä¸€ä¸ªmetas
        for i in range(len(n_steps)):
            self.metas.append(Cell(n_steps[i], n_hid, n_hid, cstr, use_norm = use_norm, use_nl = out_nl))

        #3.æ¶æ„å‚æ•°è¡¨ç¤º
        self.as_seq = []                   #* arch parameters for ops_seq(æ­£æ€åˆ†å¸ƒ)
        self.as_last_seq = []              #* arch parameters for last_seq(æ­£æ€åˆ†å¸ƒ)
        for i in range(len(n_steps)):
            if n_steps[i] > 1:
                #ai = 1e-3*[n_steps[i] - 1,n_adjs - 1](æ­£æ€åˆ†å¸ƒ)
                ai = 1e-3 * torch.randn(n_steps[i] - 1, n_adjs - 1)   #! exclude zero Op
                ai = ai.cuda()
                ai.requires_grad_(True)
                self.as_seq.append(ai)
            else:
                self.as_seq.append(None)
            #ai_last = 1e-3*[1,1]
            ai_last = 1e-3 * torch.randn(len(cstr))
            ai_last = ai_last.cuda()
            ai_last.requires_grad_(True)
            self.as_last_seq.append(ai_last)
        
        ks = [sum(1 for i in range(2, n_steps[k]) for j in range(i - 1)) for k in range(len(n_steps))]
        #as_resæ ¹æ®ksè®¾å®š
        #ai_lastæ ¹æ®n_stepsè®¾å®š
        self.as_res = []                  #* arch parameters for ops_res 
        self.as_last_res = []             #* arch parameters for last_res
        for i in range(len(n_steps)):
            if ks[i] > 0:
                ai = 1e-3 * torch.randn(ks[i], n_adjs)
                ai = ai.cuda()
                ai.requires_grad_(True)
                self.as_res.append(ai)
            else:
                self.as_res.append(None)
            
            if n_steps[i] > 1:
                #ai_last:ç”Ÿæˆä¸€ä¸ªæ­£æ€åˆ†å¸ƒçš„æ•°å­—[1,2]
                ai_last = 1e-3 * torch.randn(n_steps[i] - 1, len(cstr) + 1)
                ai_last = ai_last.cuda()
                ai_last.requires_grad_(True)
                self.as_last_res.append(ai_last)
            else:
                self.as_last_res.append(None)
        
        assert(ks[0] + n_steps[0] + (0 if self.as_last_res[0] is None else self.as_last_res[0].size(0)) == (1 + n_steps[0]) * n_steps[0] // 2)
        
        #* [optional] combine more than one meta graph? 
        self.attn_fc1 = nn.Linear(n_hid, attn_dim)
        self.attn_fc2 = nn.Linear(attn_dim, 1)
        self.fc3 = nn.Conv1d(128,128,1)

    #å°†as_seq,as_last_seq,as_res,as_last_resæ·»åŠ åˆ°alphas
    def alphas(self):
        alphas = []
        for each in self.as_seq:
            if each is not None:
                alphas.append(each)
        for each in self.as_last_seq:
            alphas.append(each)
        for each in self.as_res:
            if each is not None:
                alphas.append(each)
        for each in self.as_last_res:
            if each is not None:
                alphas.append(each)
        return alphas
    
    def sample(self, eps):
        '''
        å¯¹æ¯æ¡é“¾è·¯çš„ä¸€ä¸ªå€™é€‰è¾¹ç¼˜ç±»å‹è¿›è¡Œé‡‡æ ·
        '''
        # é‡‡æ ·as_seqæœ€å¤§å€¼çš„ç´¢å¼•
        idxes_seq = []
        # é‡‡æ ·as_resæœ€å¤§å€¼çš„ç´¢å¼•
        idxes_res = []
        if np.random.uniform() < eps:
            for i in range(len(self.metas)):
                temp = []
                temp.append(None if self.as_seq[i] is None else torch.randint(low=0, high=self.as_seq[i].size(-1), size=self.as_seq[i].size()[:-1]).cuda())
                temp.append(torch.randint(low=0, high=self.as_last_seq[i].size(-1), size=(1,)).cuda())
                idxes_seq.append(temp)
            for i in range(len(self.metas)):
                temp = []
                temp.append(None if self.as_res[i] is None else torch.randint(low=0, high=self.as_res[i].size(-1), size=self.as_res[i].size()[:-1]).cuda())
                temp.append(None if self.as_last_res[i] is None else torch.randint(low=0, high=self.as_last_res[i].size(-1), size=self.as_last_res[i].size()[:-1]).cuda())
                idxes_res.append(temp)
        else:
            for i in range(len(self.metas)):
                temp = []
                # é€‰å‡ºas_seqæ¯è¡Œæœ€å¤§å€¼çš„ç´¢å¼•
                temp.append(None if self.as_seq[i] is None else torch.argmax(F.softmax(self.as_seq[i], dim=-1), dim=-1))
                # é€‰å‡ºas_last_seqæ¯è¡Œæœ€å¤§å€¼çš„ç´¢å¼•
                temp.append(torch.argmax(F.softmax(self.as_last_seq[i], dim=-1), dim=-1))
                idxes_seq.append(temp)
            for i in range(len(self.metas)):
                temp = []
                # é€‰å‡ºas_resæ¯è¡Œæœ€å¤§å€¼çš„ç´¢å¼•
                temp.append(None if self.as_res[i] is None else torch.argmax(F.softmax(self.as_res[i], dim=-1), dim=-1))
                # é€‰å‡ºas_last_resæ¯è¡Œæœ€å¤§å€¼çš„ç´¢å¼•
                temp.append(None if self.as_last_res[i] is None else torch.argmax(F.softmax(self.as_last_res[i], dim=-1), dim=-1))
                idxes_res.append(temp)
        return idxes_seq, idxes_res
    
    def forward(self, node_feats, node_types, adjs, idxes_seq, idxes_res,semantics,cosins):
        hid = torch.zeros((node_types.size(0), self.n_hid)).cuda()
        for i in range(len(node_feats)):
            #æ¯ä¸€ä¸ªèŠ‚ç‚¹*wsæƒé‡
            hid[node_types == i] = self.ws[i](node_feats[i])
        temps = []; attns = []
        for i, meta in enumerate(self.metas):
            ws_seq = []
            ws_seq.append(None if self.as_seq[i] is None else F.softmax(self.as_seq[i], dim=-1))
            ws_seq.append(F.softmax(self.as_last_seq[i], dim=-1))
            ws_res = []
            ws_res.append(None if self.as_res[i] is None else F.softmax(self.as_res[i], dim=-1))
            ws_res.append(None if self.as_last_res[i] is None else F.softmax(self.as_last_res[i], dim=-1))
            hidi = meta(hid, adjs, ws_seq, idxes_seq[i], ws_res, idxes_res[i])
            temps.append(hidi)
            attni = self.attn_fc2(torch.tanh(self.attn_fc1(temps[-1])))
            attns.append(attni)

        #å…¨éƒ¨æ¶ˆæ¯ä¼ é€’ç»“æœæ‹¼æ¥
        hids = torch.stack(temps, dim=0).transpose(0, 1)
        #æœ€åä¸€ä¸ªæ¶ˆæ¯ä¼ é€’ç»“æœæ‹¼æ¥
        attns = F.softmax(torch.cat(attns, dim=-1), dim=-1)
        #æ¶ˆæ¯ä¼ é€’ç›¸ä¹˜
        out = (attns.unsqueeze(dim=-1) * hids).sum(dim=1)
        out = out * (1-cosins) + semantics * cosins
        # out = self.gcn(out,)
        # out = torch.cat((out,semantics),dim=1)

        return out


    def parse(self):
        '''
        å¾—å‡ºä¸€ä¸ªç”±archå‚æ•°è¡¨ç¤ºçš„å…ƒå›¾
        '''
        idxes_seq, idxes_res = self.sample(0.)
        msg_seq = []; msg_res = []
        for i in range(len(idxes_seq)):
            map_seq = [self.cstr[idxes_seq[i][1].item()]]
            #idxes_seqæ¯ä¸ªtensorç›¸åŠ 
            msg_seq.append(map_seq if idxes_seq[i][0] is None else idxes_seq[i][0].tolist() + map_seq)
            assert(len(msg_seq[i]) == self.metas[i].n_step)

            temp_res = []
            if idxes_res[i][1] is not None:
                for item in idxes_res[i][1].tolist():
                    if item < len(self.cstr):
                        temp_res.append(self.cstr[item])
                    else:
                        assert(item == len(self.cstr))
                        temp_res.append(self.n_adjs - 1)
                if idxes_res[i][0] is not None:
                    temp_res = idxes_res[i][0].tolist() + temp_res
            assert(len(temp_res) == self.metas[i].n_step * (self.metas[i].n_step - 1) // 2)
            msg_res.append(temp_res)
        
        return msg_seq, msg_res